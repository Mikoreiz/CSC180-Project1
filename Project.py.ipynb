{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "from collections.abc import Sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "\n",
    "import tensorflow.keras \n",
    "\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS ###\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tsv output file for cleaned up business json dataset\n",
    "outfile = open(\"businesses.tsv\", 'w')\n",
    "businessfile = csv.writer(outfile, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL) \n",
    "businessfile.writerow(['business_id','name', 'stars', 'category'])\n",
    "\n",
    "# Opens json dataset from path. \n",
    "with open('yelp_dataset/yelp_academic_dataset_business.json', encoding=\"utf-8\") as f: \n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        # Only getting businesses with review count over 20\n",
    "        if row['review_count'] > 20:\n",
    "            # some special char must be encoded in 'utf-8' \n",
    "            businessfile.writerow([row['business_id'], row['name'], row['stars'], row['categories']])\n",
    "\n",
    "# Close tsv file\n",
    "outfile.close()\n",
    "\n",
    "# Create pandas dataframe tsv output file\n",
    "df_business = pd.read_csv('businesses.tsv', delimiter =\"\\t\", encoding=\"utf-8\")\n",
    "df_stars = df_business[['business_id', 'stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashset for business_ids of businesses used for project\n",
    "# Will use this to search for the businesses chosen for project in the reviews json dataset\n",
    "# and add it to reviews tsv\n",
    "chosen_business = {'YZeUH6zYS0dq5QHLYZhUnQ', 'oiAlXZPIFm2nBCt0DHLu_Q', 'fNil19SUfPAPnLQrYnFrGQ', 'JjcJVqhZXhP4tvOhg3fnag'}\n",
    "\n",
    "\n",
    "#  tsv output file for cleaned up review json dataset\n",
    "outfile = open(\"review_stars.tsv\", 'w')\n",
    "sfile = csv.writer(outfile, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL) \n",
    "sfile.writerow(['business_id','stars', 'text'])\n",
    "\n",
    "# Opens json dataset from path. \n",
    "with open('yelp_dataset/yelp_academic_dataset_review.json', encoding=\"utf-8\") as f: \n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        # If statement to look for our chosen businesses\n",
    "        if row['business_id'] in chosen_business: \n",
    "            # some special char must be encoded in 'utf-8' \n",
    "            sfile.writerow([row['business_id'], row['stars'], (row['text']).encode('utf-8')])\n",
    "\n",
    "# Close tsv file\n",
    "outfile.close()\n",
    "\n",
    "# Create pandas dataframe tsv output file\n",
    "df = pd.read_csv('review_stars.tsv', delimiter =\"\\t\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all reviews for each business into one column\n",
    "df_review_agg = df.groupby('business_id')['text'].sum()\n",
    "# Merge review dataframe with star rating from business dataframe using business_id\n",
    "df_merge = pd.merge(df_review_agg, df_stars, on='business_id')\n",
    "\n",
    "print(df_merge)\n",
    "\n",
    "# Tfidf vectorizer for text column\n",
    "vectorizer = sk_text.TfidfVectorizer(\n",
    "                             stop_words='english',\n",
    "                             max_features = 1000,\n",
    "                             min_df=1)\n",
    "text_vector = vectorizer.fit_transform(df_merge['text'])\n",
    "\n",
    "# Merge vectorized text vector with dataframe\n",
    "df_text = pd.DataFrame(text_vector.toarray())\n",
    "df_concatenation = pd.concat([df_merge, df_text], axis=1)\n",
    "\n",
    "# drop unnecessary columns for neural network\n",
    "df_for_nn = df_concatenation.drop(['business_id','text'], axis=1) # drop unnecessary columns for neural network\n",
    "\n",
    "print(df_for_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = to_xy(df_for_nn,\"stars\")\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
    "model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "model.add(Dense(1)) # Output\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model.fit(x,y,verbose=2,epochs=100)    # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
