{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "from collections.abc import Sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure, show\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "from sklearn import metrics\n",
    "import tensorflow.keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping , ModelCheckpoint\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS ###\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "    \n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tsv output file for cleaned up business json dataset\n",
    "outfile = open(\"businesses.tsv\", 'w')\n",
    "businessfile = csv.writer(outfile, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL) \n",
    "businessfile.writerow(['business_id','name', 'stars', 'category'])\n",
    "\n",
    "# Opens json dataset from path. \n",
    "with open('yelp_dataset/yelp_academic_dataset_business.json', encoding=\"utf-8\") as f: \n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        # Only getting businesses with review count over 20\n",
    "        if row['review_count'] > 20:\n",
    "            # some special char must be encoded in 'utf-8' \n",
    "            businessfile.writerow([row['business_id'], row['name'], row['stars'], row['categories']])\n",
    "\n",
    "# Close tsv file\n",
    "outfile.close()\n",
    "\n",
    "# Create pandas dataframe tsv output file\n",
    "df_business = pd.read_csv('businesses.tsv', delimiter =\"\\t\", encoding=\"utf-8\")\n",
    "df_stars = df_business[['business_id', 'stars' , 'name']]\n",
    "\n",
    "print(df_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for business_ids of businesses used for project\n",
    "# Will use this to search for the businesses chosen for project in the reviews json dataset\n",
    "# and add it to reviews tsv\n",
    "#chosen_business = { 'YZeUH6zYS0dq5QHLYZhUnQ':'Hooters', 'oiAlXZPIFm2nBCt0DHLu_Q': 'Green World Cleaners', 'fNil19SUfPAPnLQrYnFrGQ' :'Cheyenne West Animal Hospital',   'JjcJVqhZXhP4tvOhg3fnag' :'Water Heater Pros' }\n",
    "\n",
    "# 5, 4, 3, 2, 1\n",
    "chosen_business = {'3C0bnFhjkgYP9mWORKg6cA':'Chili Man', 'ZkzutF0P_u0C0yTulwaHkA':'Lelulos Pizzeria', '-LfTBo0oa_uD454ScEW2XA':'Merry Anns Diner','W7hCuNdn2gzehta6eSHzgQ':'Petes Fish & Chips', 'T0NKethAB-FFR05EeZCzuA':'Burger King'}\n",
    "\n",
    "#  tsv output file for cleaned up review json dataset\n",
    "outfile = open(\"review_stars.tsv\", 'w')\n",
    "sfile = csv.writer(outfile, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL) \n",
    "sfile.writerow(['business_id','stars', 'text'])\n",
    "\n",
    "# Opens json dataset from path. \n",
    "with open('yelp_dataset/yelp_academic_dataset_review.json', encoding=\"utf-8\") as f: \n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        # If statement to look for our chosen businesses\n",
    "        if row['business_id'] in chosen_business: \n",
    "            # some special char must be encoded in 'utf-8' \n",
    "            sfile.writerow( [row['business_id'], row['stars'], (row['text']).encode('utf-8')])\n",
    "        \n",
    "            \n",
    "\n",
    "# Close tsv file\n",
    "outfile.close()\n",
    "\n",
    "# Create pandas dataframe tsv output file\n",
    "df = pd.read_csv('review_stars.tsv', delimiter =\"\\t\", encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Aggregate all reviews for each business into one column\n",
    "df_review_agg = df.groupby('business_id')['text'].sum()\n",
    "# Merge review dataframe with star rating from business dataframe using business_id\n",
    "df_merge = pd.merge(df_review_agg, df_stars, on='business_id')\n",
    "\n",
    "print(df_merge)\n",
    "\n",
    "# Tfidf vectorizer for text column\n",
    "vectorizer = sk_text.TfidfVectorizer(\n",
    "                             stop_words='english',\n",
    "                             max_features = 1000,\n",
    "                             min_df=1)\n",
    "text_vector = vectorizer.fit_transform(df_merge['text'])\n",
    "\n",
    "# Merge vectorized text vector with dataframe\n",
    "df_text = pd.DataFrame(text_vector.toarray())\n",
    "df_concatenation = pd.concat([df_merge, df_text], axis=1)\n",
    "\n",
    "business_names = []\n",
    "\n",
    "business_names = df_concatenation['name']\n",
    "# drop unnecessary columns for neural network\n",
    "df_for_nn = df_concatenation.drop(['business_id','text','name'], axis=1) # drop unnecessary columns for neural network\n",
    "\n",
    "#collumn = 0 \n",
    "#for x in range(1000):\n",
    "#    encode_numeric_zscore(df_for_nn, x)\n",
    "\n",
    "print(df_for_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = to_xy(df_for_nn,\"stars\")\n",
    "x_train, x_test, y_train, y_test = train_test_split (x, y, test_size=0.4, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hf = h5py.File('best_weights.hdf5', 'w')\n",
    "\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(500, input_dim=x.shape[1], activation='tanh'))\n",
    "# model.add(Dense(500, input_dim=x.shape[1], activation='relu'))\n",
    "# model.add(Dense(500, input_dim=x.shape[1], activation='sigmoid'))\n",
    "# model.add(Dense(250, activation='relu'))\n",
    "# model.add(Dense(125, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "#hf = h5py.File('best_weights.hdf5', 'w')\n",
    "\n",
    "# model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3, verbose=1, mode='auto')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=0, save_best_only=True) # save best model\n",
    "\n",
    "\n",
    "model.fit(x_train,y_train, validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=2,epochs=1000)    # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "\n",
    "model.load_weights('dnn/best_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(x_test)\n",
    "print(\"Shape: {}\".format(pred_test.shape))\n",
    "print(pred_test)\n",
    "\n",
    "pred_train = model.predict(x_train)\n",
    "print(pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred_test,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(pred_test.shape[0]):\n",
    "    print(\"{}. Business Name: {}, Star Rating: {}, predicted Star Rating: {}\".format(i+1,business_names[i],y[i],pred_test[i]))\n",
    "    \n",
    "for i in range(pred_train.shape[0]):\n",
    "    print(\"{}. Business Name: {}, Star Rating: {}, predicted Star Rating: {}\".format(i+3,business_names[i+2],y[i+2],pred_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out prediction\n",
    "df_y = pd.DataFrame(y_test, columns=['ground_truth'])\n",
    "df_pred = pd.DataFrame(pred_test, columns=['predicted'])\n",
    "result = pd.concat([df_y, df_pred],axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_regression(pred_test.flatten(),y_test, sort=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
